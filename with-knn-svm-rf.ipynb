{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "sparse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sezinbiner/ClassificationOfCovidXray/blob/main/with-knn-svm-rf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtnWtgnIIJ0p",
        "outputId": "b191f71b-e624-4a2d-8a53-1103d9f8979c"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxTJRvUE3FlD",
        "outputId": "a14a7fe9-7117-4e51-81db-19be54928ef9"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_dir = \"/content/gdrive/MyDrive/bitirme dataset/train\"\n",
        "test_dir = \"/content/gdrive/MyDrive/bitirme dataset/test\"\n",
        "validation_dir = \"/content/gdrive/MyDrive/bitirme dataset/validation\"\n",
        "class_names = ['covid', 'normal']\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(224, 224),  \n",
        "        batch_size=20,\n",
        "        shuffle=True,\n",
        "        class_mode='binary')\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=20,\n",
        "        shuffle=True,\n",
        "        class_mode='binary')\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=1,\n",
        "        class_mode='binary')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4800 images belonging to 2 classes.\n",
            "Found 1200 images belonging to 2 classes.\n",
            "Found 1232 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljyk1hmd31sI"
      },
      "source": [
        "def extract_features(number_of_images, generator, batch_size):\n",
        "  features = np.zeros(shape=(number_of_images, 7,7,512))\n",
        "  labels = np.zeros(shape=(number_of_images))\n",
        "  i = 0\n",
        "  print(\"loop\")\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "      print(i)\n",
        "      features_batch = extraction_layer.predict(inputs_batch)\n",
        "      features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "      labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "      i += 1\n",
        "      if i * batch_size >= number_of_images:\n",
        "          break\n",
        "  print(\"loop over\")\n",
        "  return features, labels"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5AvNZH7ISdE",
        "outputId": "b351256a-efe2-4495-b40c-13d9e426b40f"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import  InceptionV3\n",
        "import numpy as np\n",
        "\n",
        "extraction_layer = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "extraction_layer.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aitwo7k3_Ib_",
        "outputId": "45df362f-826d-48a7-c41f-c3a68baa4fe3"
      },
      "source": [
        "train_features, train_labels = extract_features(4800, train_generator, 20)\n",
        "#validation_features, validation_labels = extract_features(1200, validation_generator, 20)\n",
        "test_features, test_labels = extract_features(1232, test_generator, 1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loop\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "loop over\n",
            "loop\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "loop over\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mYV_HKDpn2K"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyeYO1phnISW"
      },
      "source": [
        "def knn(x_train,y_train,x_test,y_test):\n",
        "    rand_x = []\n",
        "    rand_test = []\n",
        "    #rand_x,rand_test = create_lbp(x_train,x_test)\n",
        "    for img in x_train:\n",
        "      t=np.ravel(img)\n",
        "      rand_x.append(t)\n",
        "    for img in x_test:\n",
        "      t=np.ravel(img)\n",
        "      rand_test.append(t)\n",
        "    pixels = pd.DataFrame(rand_x)\n",
        "    labels = pd.DataFrame(y_train)\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=20, p=2,\n",
        "                     weights='distance')\n",
        "    \n",
        "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "    knn.fit(rand_x,y_train)\n",
        "    \n",
        "    y_pred=knn.predict(rand_test)\n",
        "\n",
        "    \n",
        "    #Import scikit-learn metrics module for accuracy calculation\n",
        "    from sklearn import metrics\n",
        "    # Model Accuracy, how often is the classifier correct?\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "    \n",
        "    # from sklearn.model_selection import GridSearchCV\n",
        "    # param_grid = {'n_neighbors': [5, 10, 15, 20, 25],  \n",
        "    #           'weights': ['uniform','distance'], \n",
        "    #           'algorithm': ['auto','ball_tree','kd_tree','brute']}  \n",
        "  \n",
        "    # grid = GridSearchCV(knn, param_grid, refit = True, verbose = 3) \n",
        "  \n",
        "    # # fitting the model for grid search \n",
        "    # #grid.fit(rand_x, y_train) \n",
        "\n",
        "    \n",
        "    # model =  grid.fit(rand_x,y_train)\n",
        "    \n",
        "    # #y_pred_train = model.predict(rand_x)\n",
        "    #     # predictions for test\n",
        "    # y_pred_test = model.predict(rand_test)\n",
        "    #     # training metrics\n",
        "    #print(\"Training metrics:\")\n",
        "    # print(sklearn.metrics.classification_report(y_true= y_train, y_pred= y_pred_train))\n",
        "        \n",
        "         # test data metrics\n",
        "    print(\"Test data metrics:\")\n",
        "    print(sklearn.metrics.classification_report(y_true= y_test, y_pred= y_pred))\n",
        "    # print(\"Accuracy:\",metrics.accuracy_score(y_pred_test, y_pred))\n",
        "    # print(\"Best parameters:\")\n",
        "    # print(grid.best_params_) \n",
        "  \n",
        "    #   # print how our model looks after hyper-parameter tuning \n",
        "    # print(\"Estimator:\")\n",
        "    # print(grid.best_estimator_) \n",
        "\n",
        "    return metrics.accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr2u_vWpq143",
        "outputId": "3de7107a-b4b0-440a-c18d-689c1ef769e0"
      },
      "source": [
        "knn(train_features,train_labels,test_features,test_labels)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9512987012987013\n",
            "Test data metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.95      0.95       616\n",
            "         1.0       0.95      0.95      0.95       616\n",
            "\n",
            "    accuracy                           0.95      1232\n",
            "   macro avg       0.95      0.95      0.95      1232\n",
            "weighted avg       0.95      0.95      0.95      1232\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9512987012987013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6u0BCZvtdAj"
      },
      "source": [
        "def SVM(x_train,y_train,x_test,y_test):\n",
        "    \n",
        "    rand_x = []\n",
        "    rand_test = []\n",
        "    #rand_x,rand_test = create_lbp(x_train,x_test)\n",
        "    for img in x_train:\n",
        "      t=np.ravel(img)\n",
        "      rand_x.append(t)\n",
        "    for img in x_test:\n",
        "      t=np.ravel(img)\n",
        "      rand_test.append(t)\n",
        "    pixels = pd.DataFrame(rand_x)\n",
        "    labels = pd.DataFrame(y_train)\n",
        "    from sklearn.svm import SVC\n",
        "    svc=SVC(kernel='rbf')\n",
        "    \n",
        "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "    svc.fit(rand_x,y_train)\n",
        "    \n",
        "    y_pred=svc.predict(rand_test)\n",
        "\n",
        "    \n",
        "    #Import scikit-learn metrics module for accuracy calculation\n",
        "    from sklearn import metrics\n",
        "    # Model Accuracy, how often is the classifier correct?\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "    \n",
        "    # from sklearn.model_selection import GridSearchCV\n",
        "    # param_grid = {'C': [ 1, 10, 100],  \n",
        "    #           'gamma': [0.1, 0.01, 0.001], \n",
        "    #           'kernel': ['rbf','poly','sigmoid']} \n",
        "  \n",
        "    # grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3) \n",
        "  \n",
        "    # # fitting the model for grid search \n",
        "    # grid.fit(rand_x, y_train) \n",
        "\n",
        "    \n",
        "    # #y_pred_train = model.predict(rand_x)\n",
        "    #     # predictions for test\n",
        "    # y_pred_test = grid.predict(rand_test)\n",
        "    #     # training metrics\n",
        "    #print(\"Training metrics:\")\n",
        "    #print(sklearn.metrics.classification_report(y_true= y_train, y_pred= y_pred_train))\n",
        "        \n",
        "        # test data metrics\n",
        "    print(\"Test data metrics:\")\n",
        "    print(sklearn.metrics.classification_report(y_true= y_test, y_pred= y_pred))\n",
        "    # print(\"Accuracy:\",metrics.accuracy_score(y_pred_test, y_test))\n",
        "    # print(\"Best parameters:\")\n",
        "    # print(grid.best_params_) \n",
        "  \n",
        "    #   # print how our model looks after hyper-parameter tuning \n",
        "\n",
        "    # print(\"Estimator:\")\n",
        "    # print(grid.best_estimator_) \n",
        "\n",
        "    return metrics.accuracy_score(y_test, y_pred)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uywzJ5LjtmXR",
        "outputId": "30f488b9-65c4-4e3a-e1d6-42b048d32909"
      },
      "source": [
        "SVM(train_features,train_labels,test_features,test_labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9448051948051948\n",
            "Test data metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.94      0.94       616\n",
            "         1.0       0.94      0.95      0.95       616\n",
            "\n",
            "    accuracy                           0.94      1232\n",
            "   macro avg       0.94      0.94      0.94      1232\n",
            "weighted avg       0.94      0.94      0.94      1232\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9448051948051948"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbRfnuBYvrVY"
      },
      "source": [
        "def random_forest(x_train,y_train,x_test,y_test):\n",
        "    \n",
        "    rand_x = []\n",
        "    rand_test = []\n",
        "    #rand_x,rand_test = create_lbp(x_train,x_test)\n",
        "    for img in x_train:\n",
        "      t=np.ravel(img)\n",
        "      rand_x.append(t)\n",
        "    for img in x_test:\n",
        "      t=np.ravel(img)\n",
        "      rand_test.append(t)\n",
        "    pixels = pd.DataFrame(rand_x)\n",
        "    labels = pd.DataFrame(y_train)\n",
        "    #labels = pd.DataFrame(y_train)\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    rf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
        "                       criterion='gini', max_depth=None, max_features='auto',\n",
        "                       max_leaf_nodes=None, max_samples=None,\n",
        "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                       min_samples_leaf=2, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
        "                       oob_score=False, random_state=None, verbose=0,\n",
        "                       warm_start=False)\n",
        "    \n",
        "    #Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "    rf.fit(rand_x,y_train)\n",
        "    \n",
        "    y_pred=rf.predict(rand_test)\n",
        "\n",
        "    \n",
        "    #Import scikit-learn metrics module for accuracy calculation\n",
        "    from sklearn import metrics\n",
        "    # Model Accuracy, how often is the classifier correct?\n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "    \n",
        "    # from sklearn.model_selection import GridSearchCV\n",
        "    # clf = GridSearchCV(rf, param_grid={'n_estimators':[100,200],'min_samples_leaf':[2,3]})\n",
        "    \n",
        "    # clf.fit(rand_x,y_train)\n",
        "    \n",
        "    # #y_pred_train = clf.predict(rand_x)\n",
        "    #     # predictions for test\n",
        "    # y_pred_test = model.predict(rand_test)\n",
        "    #     # training metrics\n",
        "    # print(\"Training metrics:\")\n",
        "    #print(sklearn.metrics.classification_report(y_true= y_train, y_pred= y_pred_train))\n",
        "        \n",
        "        # test data metrics\n",
        "    print(\"Test data metrics:\")\n",
        "    print(sklearn.metrics.classification_report(y_true= y_test, y_pred= y_pred))\n",
        "    # print(\"Accuracy:\",metrics.accuracy_score(y_pred_test, y_test))\n",
        "    \n",
        "    # print(\"Best parameters:\")\n",
        "    # print(clf.best_params_)\n",
        "\n",
        "  \n",
        "    #       # print how our model looks after hyper-parameter tuning \n",
        "    # print(\"Estimator:\")\n",
        "    # print(clf.best_estimator_)\n",
        "\n",
        "    return metrics.accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62WhTokzv4Tg",
        "outputId": "201c148e-d39b-4b1f-f6b7-f06666236f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "random_forest(train_features,train_labels,test_features,test_labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.939935064935065\n",
            "Test data metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.94      0.94       616\n",
            "         1.0       0.94      0.94      0.94       616\n",
            "\n",
            "    accuracy                           0.94      1232\n",
            "   macro avg       0.94      0.94      0.94      1232\n",
            "weighted avg       0.94      0.94      0.94      1232\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.939935064935065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CvNxr3r_nko"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer= \"adam\" ,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2zX7zweAHZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1fb9d0-8957-4188-8a03-2628bb883a0c"
      },
      "source": [
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(validation_features, validation_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0670 - accuracy: 0.9596 - val_loss: 0.2685 - val_accuracy: 0.9633\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0737 - accuracy: 0.9598 - val_loss: 0.1141 - val_accuracy: 0.9583\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0855 - accuracy: 0.9471 - val_loss: 0.1844 - val_accuracy: 0.9650\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0677 - accuracy: 0.9617 - val_loss: 0.1850 - val_accuracy: 0.9658\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0647 - accuracy: 0.9565 - val_loss: 0.1976 - val_accuracy: 0.9692\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0682 - accuracy: 0.9573 - val_loss: 0.2033 - val_accuracy: 0.9642\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0597 - accuracy: 0.9642 - val_loss: 0.1614 - val_accuracy: 0.9617\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0682 - accuracy: 0.9623 - val_loss: 0.1551 - val_accuracy: 0.9625\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0650 - accuracy: 0.9608 - val_loss: 0.2731 - val_accuracy: 0.9625\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0666 - accuracy: 0.9554 - val_loss: 0.2331 - val_accuracy: 0.9658\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0852 - accuracy: 0.9431 - val_loss: 0.2251 - val_accuracy: 0.9650\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0697 - accuracy: 0.9608 - val_loss: 0.2467 - val_accuracy: 0.9650\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0711 - accuracy: 0.9548 - val_loss: 0.2021 - val_accuracy: 0.9650\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0671 - accuracy: 0.9573 - val_loss: 0.2526 - val_accuracy: 0.9608\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0819 - accuracy: 0.9396 - val_loss: 0.2086 - val_accuracy: 0.9650\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0647 - accuracy: 0.9529 - val_loss: 0.2294 - val_accuracy: 0.9608\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0602 - accuracy: 0.9640 - val_loss: 0.2354 - val_accuracy: 0.9633\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0526 - accuracy: 0.9673 - val_loss: 0.2360 - val_accuracy: 0.9642\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0537 - accuracy: 0.9671 - val_loss: 0.1924 - val_accuracy: 0.9658\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0546 - accuracy: 0.9675 - val_loss: 0.2085 - val_accuracy: 0.9675\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0582 - accuracy: 0.9567 - val_loss: 0.2060 - val_accuracy: 0.9625\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0639 - accuracy: 0.9623 - val_loss: 0.2782 - val_accuracy: 0.9633\n",
            "Epoch 23/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0573 - accuracy: 0.9671 - val_loss: 0.2040 - val_accuracy: 0.9667\n",
            "Epoch 24/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0600 - accuracy: 0.9615 - val_loss: 0.2556 - val_accuracy: 0.9658\n",
            "Epoch 25/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0680 - accuracy: 0.9519 - val_loss: 0.2553 - val_accuracy: 0.9667\n",
            "Epoch 26/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0570 - accuracy: 0.9642 - val_loss: 0.2578 - val_accuracy: 0.9633\n",
            "Epoch 27/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0673 - accuracy: 0.9490 - val_loss: 0.2408 - val_accuracy: 0.9633\n",
            "Epoch 28/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0832 - accuracy: 0.9475 - val_loss: 0.2364 - val_accuracy: 0.9658\n",
            "Epoch 29/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0608 - accuracy: 0.9558 - val_loss: 0.2949 - val_accuracy: 0.9667\n",
            "Epoch 30/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0582 - accuracy: 0.9627 - val_loss: 0.2632 - val_accuracy: 0.9650\n",
            "Epoch 31/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0578 - accuracy: 0.9642 - val_loss: 0.2717 - val_accuracy: 0.9633\n",
            "Epoch 32/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0623 - accuracy: 0.9573 - val_loss: 0.2422 - val_accuracy: 0.9600\n",
            "Epoch 33/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0711 - accuracy: 0.9533 - val_loss: 0.2432 - val_accuracy: 0.9658\n",
            "Epoch 34/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0648 - accuracy: 0.9702 - val_loss: 0.2763 - val_accuracy: 0.9633\n",
            "Epoch 35/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0659 - accuracy: 0.9575 - val_loss: 0.2275 - val_accuracy: 0.9633\n",
            "Epoch 36/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0561 - accuracy: 0.9602 - val_loss: 0.2317 - val_accuracy: 0.9550\n",
            "Epoch 37/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0551 - accuracy: 0.9631 - val_loss: 0.2915 - val_accuracy: 0.9633\n",
            "Epoch 38/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0600 - accuracy: 0.9613 - val_loss: 0.2722 - val_accuracy: 0.9633\n",
            "Epoch 39/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0682 - accuracy: 0.9513 - val_loss: 0.2895 - val_accuracy: 0.9658\n",
            "Epoch 40/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0633 - accuracy: 0.9625 - val_loss: 0.2747 - val_accuracy: 0.9642\n",
            "Epoch 41/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0543 - accuracy: 0.9635 - val_loss: 0.2949 - val_accuracy: 0.9675\n",
            "Epoch 42/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0482 - accuracy: 0.9698 - val_loss: 0.3034 - val_accuracy: 0.9692\n",
            "Epoch 43/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0559 - accuracy: 0.9615 - val_loss: 0.2791 - val_accuracy: 0.9667\n",
            "Epoch 44/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0548 - accuracy: 0.9617 - val_loss: 0.2914 - val_accuracy: 0.9683\n",
            "Epoch 45/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0724 - accuracy: 0.9554 - val_loss: 0.2123 - val_accuracy: 0.9608\n",
            "Epoch 46/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0523 - accuracy: 0.9629 - val_loss: 0.2905 - val_accuracy: 0.9658\n",
            "Epoch 47/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0635 - accuracy: 0.9583 - val_loss: 0.2557 - val_accuracy: 0.9667\n",
            "Epoch 48/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0607 - accuracy: 0.9602 - val_loss: 0.2867 - val_accuracy: 0.9667\n",
            "Epoch 49/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0712 - accuracy: 0.9585 - val_loss: 0.2699 - val_accuracy: 0.9658\n",
            "Epoch 50/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9683 - val_loss: 0.2340 - val_accuracy: 0.9667\n",
            "Epoch 51/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.1106 - accuracy: 0.9321 - val_loss: 0.2302 - val_accuracy: 0.9683\n",
            "Epoch 52/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0807 - accuracy: 0.9398 - val_loss: 0.3271 - val_accuracy: 0.9558\n",
            "Epoch 53/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0607 - accuracy: 0.9602 - val_loss: 0.2359 - val_accuracy: 0.9658\n",
            "Epoch 54/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0525 - accuracy: 0.9635 - val_loss: 0.2563 - val_accuracy: 0.9667\n",
            "Epoch 55/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0517 - accuracy: 0.9652 - val_loss: 0.3213 - val_accuracy: 0.9667\n",
            "Epoch 56/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0516 - accuracy: 0.9646 - val_loss: 0.3363 - val_accuracy: 0.9683\n",
            "Epoch 57/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0524 - accuracy: 0.9600 - val_loss: 0.2847 - val_accuracy: 0.9650\n",
            "Epoch 58/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0579 - accuracy: 0.9610 - val_loss: 0.3102 - val_accuracy: 0.9525\n",
            "Epoch 59/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0609 - accuracy: 0.9675 - val_loss: 0.2855 - val_accuracy: 0.9633\n",
            "Epoch 60/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0568 - accuracy: 0.9583 - val_loss: 0.2956 - val_accuracy: 0.9683\n",
            "Epoch 61/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0557 - accuracy: 0.9650 - val_loss: 0.2827 - val_accuracy: 0.9675\n",
            "Epoch 62/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0497 - accuracy: 0.9690 - val_loss: 0.3206 - val_accuracy: 0.9717\n",
            "Epoch 63/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0642 - accuracy: 0.9563 - val_loss: 0.3800 - val_accuracy: 0.9600\n",
            "Epoch 64/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0639 - accuracy: 0.9556 - val_loss: 0.2040 - val_accuracy: 0.9675\n",
            "Epoch 65/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0650 - accuracy: 0.9640 - val_loss: 0.2192 - val_accuracy: 0.9658\n",
            "Epoch 66/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0544 - accuracy: 0.9621 - val_loss: 0.2069 - val_accuracy: 0.9683\n",
            "Epoch 67/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0596 - accuracy: 0.9623 - val_loss: 0.2821 - val_accuracy: 0.9692\n",
            "Epoch 68/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0584 - accuracy: 0.9625 - val_loss: 0.2613 - val_accuracy: 0.9700\n",
            "Epoch 69/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0568 - accuracy: 0.9615 - val_loss: 0.2680 - val_accuracy: 0.9667\n",
            "Epoch 70/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0728 - accuracy: 0.9517 - val_loss: 0.3000 - val_accuracy: 0.9733\n",
            "Epoch 71/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0514 - accuracy: 0.9683 - val_loss: 0.3582 - val_accuracy: 0.9642\n",
            "Epoch 72/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0619 - accuracy: 0.9604 - val_loss: 0.1989 - val_accuracy: 0.9667\n",
            "Epoch 73/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0566 - accuracy: 0.9627 - val_loss: 0.2902 - val_accuracy: 0.9642\n",
            "Epoch 74/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0600 - accuracy: 0.9573 - val_loss: 0.2792 - val_accuracy: 0.9658\n",
            "Epoch 75/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0511 - accuracy: 0.9625 - val_loss: 0.3659 - val_accuracy: 0.9683\n",
            "Epoch 76/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0515 - accuracy: 0.9654 - val_loss: 0.3401 - val_accuracy: 0.9642\n",
            "Epoch 77/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0557 - accuracy: 0.9658 - val_loss: 0.3208 - val_accuracy: 0.9700\n",
            "Epoch 78/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0532 - accuracy: 0.9692 - val_loss: 0.2947 - val_accuracy: 0.9658\n",
            "Epoch 79/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0585 - accuracy: 0.9671 - val_loss: 0.2429 - val_accuracy: 0.9650\n",
            "Epoch 80/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0501 - accuracy: 0.9642 - val_loss: 0.3056 - val_accuracy: 0.9650\n",
            "Epoch 81/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0559 - accuracy: 0.9590 - val_loss: 0.3181 - val_accuracy: 0.9650\n",
            "Epoch 82/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0496 - accuracy: 0.9660 - val_loss: 0.2810 - val_accuracy: 0.9642\n",
            "Epoch 83/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0574 - accuracy: 0.9579 - val_loss: 0.3196 - val_accuracy: 0.9667\n",
            "Epoch 84/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0594 - accuracy: 0.9638 - val_loss: 0.2740 - val_accuracy: 0.9633\n",
            "Epoch 85/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0560 - accuracy: 0.9619 - val_loss: 0.3210 - val_accuracy: 0.9675\n",
            "Epoch 86/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0558 - accuracy: 0.9613 - val_loss: 0.3292 - val_accuracy: 0.9667\n",
            "Epoch 87/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0568 - accuracy: 0.9583 - val_loss: 0.3042 - val_accuracy: 0.9650\n",
            "Epoch 88/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0549 - accuracy: 0.9663 - val_loss: 0.2843 - val_accuracy: 0.9617\n",
            "Epoch 89/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0505 - accuracy: 0.9646 - val_loss: 0.3617 - val_accuracy: 0.9658\n",
            "Epoch 90/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0529 - accuracy: 0.9633 - val_loss: 0.3009 - val_accuracy: 0.9625\n",
            "Epoch 91/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0830 - accuracy: 0.9544 - val_loss: 0.2474 - val_accuracy: 0.9642\n",
            "Epoch 92/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0589 - accuracy: 0.9581 - val_loss: 0.2926 - val_accuracy: 0.9633\n",
            "Epoch 93/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0467 - accuracy: 0.9685 - val_loss: 0.3255 - val_accuracy: 0.9650\n",
            "Epoch 94/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0722 - accuracy: 0.9548 - val_loss: 0.3188 - val_accuracy: 0.9667\n",
            "Epoch 95/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0516 - accuracy: 0.9663 - val_loss: 0.3453 - val_accuracy: 0.9625\n",
            "Epoch 96/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0641 - accuracy: 0.9548 - val_loss: 0.2914 - val_accuracy: 0.9650\n",
            "Epoch 97/100\n",
            "300/300 [==============================] - 1s 3ms/step - loss: 0.0567 - accuracy: 0.9629 - val_loss: 0.2884 - val_accuracy: 0.9658\n",
            "Epoch 98/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0488 - accuracy: 0.9640 - val_loss: 0.3618 - val_accuracy: 0.9650\n",
            "Epoch 99/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0545 - accuracy: 0.9671 - val_loss: 0.3022 - val_accuracy: 0.9667\n",
            "Epoch 100/100\n",
            "300/300 [==============================] - 1s 4ms/step - loss: 0.0535 - accuracy: 0.9704 - val_loss: 0.2849 - val_accuracy: 0.9675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5OmL2alWWAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c567a2-7d42-40c7-adf1-0cdb67bc0d40"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               3211392   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 3,211,521\n",
            "Trainable params: 3,211,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmxWgsP-CSSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "97a4c991-5e49-4211-ed42-041872e17159"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "f, axes = plt.subplots(1,2,figsize=(14,4))\n",
        "\n",
        "axes[0].plot(epochs, acc, 'bo', label='Training acc')\n",
        "axes[0].plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(epochs, loss, 'bo', label='Training loss')\n",
        "axes[1].plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "axes[1].yaxis.set_label_position(\"right\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxPc1HmDve-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79dc8791-5f80-4ac1-f646-1749196d314c"
      },
      "source": [
        "import tensorflow as tf\n",
        "labels=[]\n",
        "y_pred=model.predict(validation_features)\n",
        "for prob in y_pred:\n",
        "  if prob < 0.5:\n",
        "    labels.append(0)\n",
        "  else:\n",
        "    labels.append(1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(validation_labels, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[579,  21],\n",
              "       [ 18, 582]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5br6YcBKcaM",
        "outputId": "6297f529-4331-4859-bbb9-d1209ab5d260"
      },
      "source": [
        "test_features, test_labels = extract_features(1232, test_generator, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loop\n",
            "loop over\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra1ImmsyCwOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1289021d-9cec-4b01-b9e2-8abd164326f4"
      },
      "source": [
        "from sklearn import metrics\n",
        "y_pred = model.predict(test_features)\n",
        "probas = np.array(y_pred)\n",
        "labels = []\n",
        "for prob in probas:\n",
        "  if prob < 0.5:\n",
        "    labels.append(0)\n",
        "  else:\n",
        "    labels.append(1)\n",
        "y_true_labels = test_labels\n",
        "y_pred_labels = labels\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_true_labels, y_pred_labels,pos_label='positive', average='micro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_true_labels, y_pred_labels, average='micro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
        "print('F1 score: %f' % f1)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=[\"covid\", \"normal\"]))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true_labels, y_pred_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.972403\n",
            "Precision: 0.972403\n",
            "Recall: 0.972403\n",
            "F1 score: 0.972397\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       covid       0.99      0.96      0.97       616\n",
            "      normal       0.96      0.99      0.97       616\n",
            "\n",
            "    accuracy                           0.97      1232\n",
            "   macro avg       0.97      0.97      0.97      1232\n",
            "weighted avg       0.97      0.97      0.97      1232\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1321: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  % (pos_label, average), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[590,  26],\n",
              "       [  8, 608]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Vus5RfFTEw"
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.inception_v3 import  InceptionV3\n",
        "from tensorflow.python.keras.applications.nasnet import NASNetMobile\n",
        "import numpy as np\n",
        "\n",
        "extraction_layer = NASNetMobile(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "extraction_layer.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EILeAHmGT7r",
        "outputId": "12914ac1-753a-4a0a-d4b5-abaf6788d978"
      },
      "source": [
        "train_features, train_labels = extract_features(4800, train_generator, 20)\n",
        "validation_features, validation_labels = extract_features(1200, validation_generator, 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loop\n",
            "loop over\n",
            "loop\n",
            "loop over\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aewLpgl9GhE0"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import models\n",
        "from keras import layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer= \"adam\" ,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgna7g0lHdNF"
      },
      "source": [
        "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
        "learn_control = ReduceLROnPlateau(monitor='accuracy', patience=5,\n",
        "                                  verbose=1,factor=0.2, min_lr=1e-7)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('/content/gdrive/My Drive/models/nasnasnet.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9xnGPeTGkUB",
        "outputId": "eaae99e7-98be-4116-c717-e8d160999dfb"
      },
      "source": [
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=100,\n",
        "                    batch_size=16,\n",
        "                    validation_data=(validation_features, validation_labels),\n",
        "                    callbacks=[learn_control, model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "300/300 [==============================] - 2s 6ms/step - loss: 0.0588 - accuracy: 0.9769 - val_loss: 0.2814 - val_accuracy: 0.9658\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.28140, saving model to /content/gdrive/My Drive/models/nasnasnet.h5\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0655 - accuracy: 0.9754 - val_loss: 0.2889 - val_accuracy: 0.9533\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.28140\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0583 - accuracy: 0.9769 - val_loss: 0.3070 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.28140\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0547 - accuracy: 0.9796 - val_loss: 0.2716 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.28140 to 0.27158, saving model to /content/gdrive/My Drive/models/nasnasnet.h5\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0552 - accuracy: 0.9756 - val_loss: 0.3356 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.27158\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0553 - accuracy: 0.9790 - val_loss: 0.3300 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.27158\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0619 - accuracy: 0.9744 - val_loss: 0.3464 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.27158\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0560 - accuracy: 0.9769 - val_loss: 0.2832 - val_accuracy: 0.9625\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.27158\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0655 - accuracy: 0.9717 - val_loss: 0.3733 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.27158\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0562 - accuracy: 0.9767 - val_loss: 0.3779 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.27158\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0525 - accuracy: 0.9783 - val_loss: 0.3641 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.27158\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0511 - accuracy: 0.9790 - val_loss: 0.3513 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.27158\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0495 - accuracy: 0.9790 - val_loss: 0.3335 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.27158\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0446 - accuracy: 0.9827 - val_loss: 0.3790 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.27158\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0516 - accuracy: 0.9781 - val_loss: 0.3742 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.27158\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0454 - accuracy: 0.9819 - val_loss: 0.4083 - val_accuracy: 0.9583\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.27158\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0405 - accuracy: 0.9835 - val_loss: 0.4043 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27158\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0447 - accuracy: 0.9840 - val_loss: 0.3540 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.27158\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0450 - accuracy: 0.9821 - val_loss: 0.4732 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.27158\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0447 - accuracy: 0.9823 - val_loss: 0.3875 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27158\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0443 - accuracy: 0.9829 - val_loss: 0.3467 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.27158\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0442 - accuracy: 0.9825 - val_loss: 0.4129 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.27158\n",
            "Epoch 23/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0461 - accuracy: 0.9831 - val_loss: 0.4311 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.27158\n",
            "Epoch 24/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0398 - accuracy: 0.9848 - val_loss: 0.4199 - val_accuracy: 0.9592\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.27158\n",
            "Epoch 25/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0432 - accuracy: 0.9827 - val_loss: 0.4092 - val_accuracy: 0.9633\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.27158\n",
            "Epoch 26/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0423 - accuracy: 0.9823 - val_loss: 0.4150 - val_accuracy: 0.9625\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.27158\n",
            "Epoch 27/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0405 - accuracy: 0.9840 - val_loss: 0.4259 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.27158\n",
            "Epoch 28/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0544 - accuracy: 0.9829 - val_loss: 0.4109 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.27158\n",
            "Epoch 29/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0403 - accuracy: 0.9837 - val_loss: 0.4003 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.27158\n",
            "Epoch 30/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0443 - accuracy: 0.9808 - val_loss: 0.4021 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.27158\n",
            "Epoch 31/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0413 - accuracy: 0.9844 - val_loss: 0.4046 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.27158\n",
            "Epoch 32/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0430 - accuracy: 0.9823 - val_loss: 0.4077 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.27158\n",
            "Epoch 33/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0403 - accuracy: 0.9846 - val_loss: 0.4072 - val_accuracy: 0.9642\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.27158\n",
            "Epoch 34/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0431 - accuracy: 0.9833 - val_loss: 0.4094 - val_accuracy: 0.9617\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.27158\n",
            "Epoch 35/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0387 - accuracy: 0.9852 - val_loss: 0.4097 - val_accuracy: 0.9625\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.27158\n",
            "Epoch 36/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0441 - accuracy: 0.9831 - val_loss: 0.4089 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.27158\n",
            "Epoch 37/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0425 - accuracy: 0.9827 - val_loss: 0.4104 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.27158\n",
            "Epoch 38/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0457 - accuracy: 0.9829 - val_loss: 0.4106 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.27158\n",
            "Epoch 39/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0416 - accuracy: 0.9842 - val_loss: 0.4118 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.27158\n",
            "Epoch 40/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0386 - accuracy: 0.9858 - val_loss: 0.4121 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.27158\n",
            "Epoch 41/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0408 - accuracy: 0.9837 - val_loss: 0.4128 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.27158\n",
            "Epoch 42/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0398 - accuracy: 0.9850 - val_loss: 0.4139 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.27158\n",
            "Epoch 43/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0423 - accuracy: 0.9837 - val_loss: 0.4137 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.27158\n",
            "Epoch 44/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0399 - accuracy: 0.9852 - val_loss: 0.4141 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.27158\n",
            "Epoch 45/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0418 - accuracy: 0.9846 - val_loss: 0.4146 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00045: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.27158\n",
            "Epoch 46/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0368 - accuracy: 0.9860 - val_loss: 0.4147 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27158\n",
            "Epoch 47/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0427 - accuracy: 0.9821 - val_loss: 0.4146 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.27158\n",
            "Epoch 48/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0386 - accuracy: 0.9856 - val_loss: 0.4146 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27158\n",
            "Epoch 49/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0440 - accuracy: 0.9835 - val_loss: 0.4149 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.27158\n",
            "Epoch 50/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0427 - accuracy: 0.9837 - val_loss: 0.4150 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.27158\n",
            "Epoch 51/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0450 - accuracy: 0.9817 - val_loss: 0.4152 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.27158\n",
            "Epoch 52/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0410 - accuracy: 0.9840 - val_loss: 0.4152 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.27158\n",
            "Epoch 53/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0452 - accuracy: 0.9810 - val_loss: 0.4153 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.27158\n",
            "Epoch 54/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0410 - accuracy: 0.9844 - val_loss: 0.4153 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.27158\n",
            "Epoch 55/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0400 - accuracy: 0.9844 - val_loss: 0.4153 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.27158\n",
            "Epoch 56/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0390 - accuracy: 0.9844 - val_loss: 0.4153 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.27158\n",
            "Epoch 57/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0386 - accuracy: 0.9858 - val_loss: 0.4153 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.27158\n",
            "Epoch 58/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0353 - accuracy: 0.9858 - val_loss: 0.4153 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.27158\n",
            "Epoch 59/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0432 - accuracy: 0.9821 - val_loss: 0.4154 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.27158\n",
            "Epoch 60/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0438 - accuracy: 0.9821 - val_loss: 0.4154 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.27158\n",
            "Epoch 61/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0377 - accuracy: 0.9867 - val_loss: 0.4154 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.27158\n",
            "Epoch 62/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0392 - accuracy: 0.9865 - val_loss: 0.4155 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.27158\n",
            "Epoch 63/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0463 - accuracy: 0.9802 - val_loss: 0.4155 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.27158\n",
            "Epoch 64/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0493 - accuracy: 0.9794 - val_loss: 0.4156 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.27158\n",
            "Epoch 65/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0433 - accuracy: 0.9819 - val_loss: 0.4156 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.27158\n",
            "Epoch 66/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0448 - accuracy: 0.9821 - val_loss: 0.4156 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.27158\n",
            "Epoch 67/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0457 - accuracy: 0.9804 - val_loss: 0.4157 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.27158\n",
            "Epoch 68/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0425 - accuracy: 0.9842 - val_loss: 0.4157 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.27158\n",
            "Epoch 69/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0401 - accuracy: 0.9844 - val_loss: 0.4157 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.27158\n",
            "Epoch 70/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0378 - accuracy: 0.9856 - val_loss: 0.4157 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.27158\n",
            "Epoch 71/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0454 - accuracy: 0.9829 - val_loss: 0.4158 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.27158\n",
            "Epoch 72/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0429 - accuracy: 0.9827 - val_loss: 0.4158 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.27158\n",
            "Epoch 73/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0389 - accuracy: 0.9858 - val_loss: 0.4158 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.27158\n",
            "Epoch 74/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0423 - accuracy: 0.9831 - val_loss: 0.4159 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.27158\n",
            "Epoch 75/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0426 - accuracy: 0.9829 - val_loss: 0.4159 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.27158\n",
            "Epoch 76/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0460 - accuracy: 0.9806 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.27158\n",
            "Epoch 77/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0433 - accuracy: 0.9833 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.27158\n",
            "Epoch 78/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0433 - accuracy: 0.9829 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.27158\n",
            "Epoch 79/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0372 - accuracy: 0.9856 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.27158\n",
            "Epoch 80/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0415 - accuracy: 0.9842 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.27158\n",
            "Epoch 81/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0452 - accuracy: 0.9829 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.27158\n",
            "Epoch 82/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0412 - accuracy: 0.9842 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.27158\n",
            "Epoch 83/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0448 - accuracy: 0.9810 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.27158\n",
            "Epoch 84/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0435 - accuracy: 0.9821 - val_loss: 0.4159 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.27158\n",
            "Epoch 85/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0387 - accuracy: 0.9854 - val_loss: 0.4159 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.27158\n",
            "Epoch 86/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0382 - accuracy: 0.9852 - val_loss: 0.4159 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.27158\n",
            "Epoch 87/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0452 - accuracy: 0.9823 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.27158\n",
            "Epoch 88/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0393 - accuracy: 0.9852 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.27158\n",
            "Epoch 89/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0420 - accuracy: 0.9831 - val_loss: 0.4161 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.27158\n",
            "Epoch 90/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0435 - accuracy: 0.9819 - val_loss: 0.4160 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.27158\n",
            "Epoch 91/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0398 - accuracy: 0.9842 - val_loss: 0.4161 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.27158\n",
            "Epoch 92/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0510 - accuracy: 0.9787 - val_loss: 0.4161 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.27158\n",
            "Epoch 93/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0365 - accuracy: 0.9862 - val_loss: 0.4161 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.27158\n",
            "Epoch 94/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0461 - accuracy: 0.9804 - val_loss: 0.4161 - val_accuracy: 0.9600\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.27158\n",
            "Epoch 95/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0480 - accuracy: 0.9792 - val_loss: 0.4161 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.27158\n",
            "Epoch 96/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0406 - accuracy: 0.9840 - val_loss: 0.4161 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.27158\n",
            "Epoch 97/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0415 - accuracy: 0.9840 - val_loss: 0.4162 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.27158\n",
            "Epoch 98/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0434 - accuracy: 0.9823 - val_loss: 0.4162 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.27158\n",
            "Epoch 99/100\n",
            "300/300 [==============================] - 2s 5ms/step - loss: 0.0430 - accuracy: 0.9831 - val_loss: 0.4162 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.27158\n",
            "Epoch 100/100\n",
            "300/300 [==============================] - 1s 5ms/step - loss: 0.0403 - accuracy: 0.9848 - val_loss: 0.4162 - val_accuracy: 0.9608\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.27158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOm3IKVMG-j5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "f, axes = plt.subplots(1,2,figsize=(14,4))\n",
        "\n",
        "axes[0].plot(epochs, acc, 'bo', label='Training acc')\n",
        "axes[0].plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(epochs, loss, 'bo', label='Training loss')\n",
        "axes[1].plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "axes[1].yaxis.set_label_position(\"right\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "851Iam3CHGUd"
      },
      "source": [
        "test_features, test_labels = extract_features(1232, test_generator, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjzvGdMLHDQu"
      },
      "source": [
        "from sklearn import metrics\n",
        "y_pred = model.predict(test_features)\n",
        "probas = np.array(y_pred)\n",
        "labels = []\n",
        "for prob in probas:\n",
        "  if prob < 0.5:\n",
        "    labels.append(0)\n",
        "  else:\n",
        "    labels.append(1)\n",
        "y_true_labels = test_labels\n",
        "y_pred_labels = labels\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(y_true_labels, y_pred_labels,pos_label='positive', average='micro')\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(y_true_labels, y_pred_labels, average='micro')\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')\n",
        "print('F1 score: %f' % f1)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=[\"covid\", \"normal\"]))\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_true_labels, y_pred_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}